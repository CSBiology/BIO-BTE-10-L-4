[{"uri":"/BIO-BTE-10-L-4/index.html","title":"BIO-BTE-10-L-4\n","content":"# BIO-BTE-10-L-4\n\n\u003Cbr\u003E\n\nDeutscher titel: **Wissenschaftliche Programmierung f\u00FCr Biolog*innen (Projektarbeit)**\n\nEnglish title: **Scientific Programming For Biologists (project work)**\n\nKIS: [BIO-BTE-10-L-4](https://www.kis.uni-kl.de/campus/all/event.asp?gguid=0xE21F6D88E774426AAF47627AB6FC59BB\u0026tguid=0xA0CC12CC38514E09833533E643742D94)\n\n**Credit Points (CP): 9**\n\n**Vorraussetzungen/Requirements:** [BIO-BTE-12-V-4](https://csbiology.github.io/BIO-BTE-12-V-4/)\n\n**Table of contents**\n\n\u003C!-- TOC --\u003E\n\n- [Kursbeschreibung(deutsch)](#kursbeschreibungdeutsch)\n    - [Ablauf](#ablauf)\n- [Course description(english)](#course-descriptionenglish)\n    - [Procedure](#procedure)\n\n\u003C!-- /TOC --\u003E\n\n## Kursbeschreibung(deutsch)\n\nSetzen Sie Ihre F\u00E4higkeiten mit einer individuellen Projektarbeit im Bereich der Bioinformatik in die Praxis um. M\u00F6gliche Projektthemen erstrecken sich \u00FCber das gesamte Spektrum der Bioinformatik von Algorithmus- bis zur Webdienstimplementierung. Besprechen sie ihre eigene Themenidee oder w\u00E4hlen Sie ein vorgeschlagenes Projekt f\u00FCr sich aus.\n\n### Ablauf\n\n- **Themenwahl**: Die angebotenen Projektarbeitsthemen k\u00F6nnen sie der Sidebar links entnehmen. Die jeweiligen Artikel geben bereits einen detailierten \u00DCberblick \u00FCber das Thema und verschiedene Hinweise und Tipps f\u00FCr deren Bearbeitung. \n\n- **Themenbearbeitung**: Nach einem Einf\u00FChrungsgespr\u00E4ch bearbeiten sie das Thema digital in Eigenregie. Wir stehen Ihnen dabei nat\u00FCrlich bei technischen und/oder Verst\u00E4ndnisproblemen zur Verf\u00FCgung. \n\n    _[Optional]_ Um Ihre Arbeit zu versionieren und leichter teilbar zu machen, empfiehlt sich die Einrichtung und Pflege eines [Repositories auf GitHub](https://guides.github.com/activities/hello-world/). Bei Interesse kann hier auch eine Einf\u00FChrung unsererseits erfolgen.\n\n- **Leistungsnachweis**: Der Leistungsnachweis erfolgt durch eine Nachbesprechung und ein Protokoll mitsamt Dokumentation und Anwendungsbeispiel des Projekts. Art und Pr\u00E4sentation des Protokolls kann abh\u00E4ngig vom Projekt leicht unterschiedlich sein, grunds\u00E4tzlich sollten Folgende Punkte aber immer beachtet werden:\n    - Einleitender Text zu Problemstellung, Themenfeld, etc. (hier gerne an der Projektvorstellung orientieren)\n    - Schritt-f\u00FCr-Schritt Erkl\u00E4rung der Implementation der Probleml\u00F6sung **mit Codebeispielen** im [_Blogpost Stil_]()\n    - Anwendungsbeispiel **mit Codebeispielen**\n\n## Course description(english)\n\nPut your skills into practice by working on an individual project work in the field of bioinformatics. Project topics span the range of bioinformatic application from algorithms to web service implementation. Discuss your own ideas as project topic or select a suggested project.\n\n### Procedure\n\ncoming soon"},{"uri":"/BIO-BTE-10-L-4/projects/hClust.html","title":"efficient agglomerative hierarchical clustering","content":"(**\n---\ntitle: efficient agglomerative hierarchical clustering\ncategory: projects\ncategoryindex: 1\nindex: 2\n---\n*)\n\n\n(**\n\n\n# Implementation of an efficient hierarchical agglomerative clustering algorithm\n\n\n## Content\n\n1.\tIntroduction\n\n2.\tCoding clues\n\n3.\tReferences\n\n4.\tGoal/Additional information\n\n\n\n## Introduction\n\n![](../img/overview.png)\n\nFig 1: Generating a hierarchical tree structure from a complex data set. Vertical thresholds (yellow, green, violet) generate different cluster numbers.\n\nClustering methods can be used to group elements of a huge data set based on their similarity. Elements sharing similar properties cluster together and can be \nreported as coherent group. These properties could be e.g. (a) similar gene expression kinetics in time series, (b) similar physicochemical properties, (c) genetic \nsimilarity measures for phylogenetic trees, etc.\n\nMany clustering algorithms require a predefined cluster number, that has to be provided by the experimenter. The most common approach is _k_-means clustering, \nwhere _k_ stands for the user defined cluster number. This kind of user interaction can lead to results, that are not objective, but highly influenced by the \nview and expectation of the experimenter. \n\nHierarchical clustering (_hClust_) does not require such cluster number definition. Instead, _hClust_ reports all possible cluster numbers \n(One big cluster with all elements to n clusters where every element is a singleton in its own cluster) in a tree structure (Fig 1). \nA _hClust_ tree has a single cluster (node) on its root and recursively splits up into clusters of elements that are more similar to each other than \nto elements of other clusters. For generating multiple cluster results with different number of clusters, the clustering has to performed only once. \nSubsequently the tree can be cut at any vertical line which will result in a defined number of clusters.\n\nThere are two types of _hClust_: \n\n  - Agglomerative (bottom-up): Each data point is in its own cluster and the nearest ones are merged recursively. It is referred to agglomerative hierarchical clustering (_HAC_)\n\n  - Divisive (top-down): All data points are in the same cluster and you divide the cluster into two that are far away from each other.\n\n  - The presented implementation is an agglomerative type.\n\n\nThere are several distance metrics, that can be used as distance function. The commonly used one probably is Euclidean distance. By inverting the distance, you end up with a similarity. High similarities indicate low distances, and vice versa. By calculating the similarities for every element pair, a similarity matrix can be generated.\n\n![](../img/simMatrix.png)\n\nFig 2: Data matrix (left) with measurement types as columns and (biological) entities as rows. The data matrix can be converted into a similarity matrix, that contain the inverse of distances.\n\n![](../img/workflow.png)\n\nFig 3: Workflow as proposed in pseudo code in Reference#2. \n\n## References\n\n- https://www.youtube.com/watch?v=7xHsRkOdVwo\n\n- https://github.com/srirambaskaran/efficient-hierarchical-clustering\n\n- https://nlp.stanford.edu/IR-book/pdf/17hier.pdf\n\n- https://medium.com/machine-learning-researcher/clustering-k-mean-and-hierarchical-cluster-fa2de08b4a4b\n\n\n\n## Coding clues\n\n### 0\u003Csup\u003Eth\u003C/sup\u003E step: \n\n  - Inform yourself about \n\n    - queues and priority queues (roughly)\n\n    - similarity measurements such as Euclidean distance, Manhattan distance, the advantage to use the squared Euclidean distance\n\n    - single linkage, complete linkage, and centroid based linkage types\n\n  - Down below you can see the pseudo code (not F#!) the efficient agglomerative hierarchical clustering (_HAC_) is based on:\n\n    \u0060\u0060\u0060\n    // Generating priority queue\n    Q = [] //priority queue \n    for n = 1 to N \n        for i = 1 to N \n            Q.enqueue(SIM(d[i], d[n]), (i, n)) \n    \n    // iterative agglomerative clustering\n    for k = 1 to N-1 \n        \u003Ci,m\u003E = Q.dequeue() \n        mergedCluster = merge((i,m)) \n    \n        Q.remove((_,m)) //remove any similarity that includes m \n    \n        for j = 1 to N \n            Q.update((i,j), SIM(mergedCluster, d[j])) \n    \u0060\u0060\u0060\n\n\n### 1\u003Csup\u003Est\u003C/sup\u003E step: \n\n  - create a F# script (.fsx), load and open \u0060\u0060\u0060FSharp.Stats\u0060\u0060\u0060, \u0060\u0060\u0060FSharpAux\u0060\u0060\u0060 and \u0060\u0060\u0060FSharpx.Collections\u0060\u0060\u0060\n\n  - import test data\n\n    - You can find the classic clustering dataset \u0022iris\u0022 [here](https://github.com/fslaborg/FSharp.Stats/tree/developer/docs/data).\n\n  - An implementation of an priority queue is given below.\n*)\n\n(******)\n\n#r \u0022nuget: FSharp.Stats, 0.4.1\u0022\n#r \u0022nuget: FSharpAux, 1.0.0\u0022\n#r \u0022nuget: FSharpx.Collections, 2.1.3\u0022\n#r \u0022nuget: Plotly.NET, 2.0.0-beta9\u0022\n            \nopen FSharp.Stats\nopen FSharpAux\nopen FSharpx.Collections\nopen Plotly.NET\n\n                \nlet lables,data =\n    let fromFileWithSep (separator:char) (filePath) =     \n        // The function is implemented using a sequence expression\n        seq {   let sr = System.IO.File.OpenText(filePath)\n                while not sr.EndOfStream do \n                    let line = sr.ReadLine() \n                    let words = line.Split separator//[|\u0027,\u0027;\u0027 \u0027;\u0027\\t\u0027|] \n                    yield words }\n    fromFileWithSep \u0027,\u0027 (__SOURCE_DIRECTORY__ \u002B \u0022../content/irisData.csv\u0022)\n    |\u003E Seq.skip 1\n    |\u003E Seq.map (fun arr -\u003E arr.[4], [| float arr.[0]; float arr.[1]; float arr.[2]; float arr.[3]; |])\n    |\u003E Seq.toArray\n    |\u003E FSharp.Stats.Array.shuffleFisherYates\n    |\u003E Array.mapi (fun i (lable,data) -\u003E sprintf \u0022%s_%i\u0022 lable i, data)\n    |\u003E Array.unzip\n\n    \ntype PriorityQueue\u003C\u0027T when \u0027T : comparison\u003E(values : \u0027T [], comparisonF : \u0027T -\u003E float) = \n        \n    let sort = Array.sortByDescending comparisonF\n    let mutable data = sort values \n    \n    new (comparisonF) = PriorityQueue(Array.empty,comparisonF)\n    \n    interface System.Collections.IEnumerable with\n        member this.GetEnumerator() = data.GetEnumerator()\n    \n    member this.UpdateElement (t:\u0027T) (newt:\u0027T) =     \n        let updated =\n            data \n            |\u003E Array.map (fun x -\u003E if x = t then newt else t)\n            |\u003E sort\n        data \u003C- updated\n          \n    member this.Elements = data\n        \n    member this.RemoveElement (t:\u0027T) = \n        let filtered = \n            Array.filter (fun x -\u003E x \u003C\u003E t) data\n        data \u003C- filtered\n    \n    member this.GetHead :\u0027T = \n        Array.head data\n    \n    member this.Dequeue() = \n        let head,tail = Array.head data, Array.tail data\n        data \u003C- tail\n        head, this\n    \n    member this.Insert (t:\u0027T) = \n        let newd = Array.append data [|t|] |\u003E sort\n        data \u003C- newd\n\n    member this.UpdateBy (updateElementFunction: \u0027T -\u003E \u0027T) = \n        let newd = \n            Array.map updateElementFunction data \n            |\u003E sort\n        data \u003C- newd\n\n    member this.RemoveElementsBy (predicate: \u0027T -\u003E bool) = \n        let newd = \n            Array.filter predicate data \n        data \u003C- newd\n\n(**\n### 2\u003Csup\u003End\u003C/sup\u003E step: Generate priority queue\n\n  - For each data point calculate the distances to each of the other points. \n\n    - You can find different kinds of distance measures in \u0060\u0060\u0060ML.DistanceMetrics\u0060\u0060\u0060\n\n    - Similarity can be interpreted as inverse distance. The lower the distance, the higher the similarity and the faster the data points have to be merged. \n    An appropriate type to store the result could be the following:\n\n*)\n\n(******)\n\n/// Type to store similarities\ntype Neighbour = {\n    /// inverse of distance\n    Similarity: float\n    /// list of source cluster indices\n    Source  : int list\n    /// list of target cluster indices\n    Target  : int list\n    }\n    with static\n            member Create d s t = { Similarity = d; Source = s; Target = t}\n\n//Example queue\nlet neighbours = \n    [|\n    Neighbour.Create 1. [1]      [2]\n    Neighbour.Create 2. [0]      [6]\n    Neighbour.Create 5. [3]     [5]\n    Neighbour.Create 2. [4;7;10] [8;9]\n    Neighbour.Create 7. [1]      [2]\n    |]\n\n////// usage of PriorityQueue\nlet myPQueue = PriorityQueue(neighbours,fun x -\u003E x.Similarity)\nmyPQueue.GetHead                                                                     // reports queues\nmyPQueue.RemoveElement (Neighbour.Create 5. [3] [5])                                 // removes element from queue\nmyPQueue.UpdateElement (Neighbour.Create 2. [0] [6]) (Neighbour.Create 200. [0] [6]) // update element in queue \nmyPQueue.RemoveElementsBy (fun x -\u003E not (List.contains 3 x.Source))                  // update element in queue \nmyPQueue.UpdateBy (fun x -\u003E if x.Similarity \u003E 2. then Neighbour.Create 100. x.Source x.Target else x)// update elements in queue  by given function\n\n////// usage of IntervalHeap\n#r \u0022nuget: C5, 2.5.3\u0022\nopen C5\nlet myHeap : IntervalHeap\u003CNeighbour\u003E = IntervalHeap(MemoryType.Normal)\n\nmyHeap.AddAll(neighbours)                   // adds array of neighbours\nlet max = myHeap.FindMax()                  // finds max value entry\nmyHeap.DeleteMax()                          // deletes max value entry \nmyHeap.Filter (fun x -\u003E x.Similarity = 5.)  // filters entries based on predicate function\n\n\n(**\n  - Some example applications of the PriorityQueue type are shown above.\n  \n  - Generate a priority queue that is descending regarding the similarity. \n\n\n### 3\u003Csup\u003Erd\u003C/sup\u003E step:\n  - Create a clustering list, that contains information of the current clustering state. This could be an \u0060\u0060\u0060int list []\u0060\u0060\u0060 where each of the lists contains indices of clustered data points. Since in the beginning all data points are in its own cluster the clustering list could look as follows: \n\n    - \u0060\u0060\u0060let clusteringList = [|[0];[1];[2];...[n-1]|]\u0060\u0060\u0060\n\n  - When cluster 1 and 2 merge, the clustering list may look like this:\n\n    - \u0060\u0060\u0060let clusteringList = [|[0];[1;2];[];...[n-1]|]\u0060\u0060\u0060\n\n### 4\u003Csup\u003Eth\u003C/sup\u003E step:\n  - Now the agglomeration starts. Since every data point is in its own cluster, you can perform n-1 agglomeration (merging) steps before you result in a single cluster that contains all data points.\n\n  - For each merge (1..n-1) do\n\n    - take the first entry of the priority queue (the most similar clusters)\n\n      - source indices = [i] \n\n      - target indices = [j]\n\n    - Create a new cluster, that contains the merged indices: [i;j]\n\n    - Save the new cluster configuration in your clustering list. Therefore you can add j to the i\u003Csup\u003Eth\u003C/sup\u003E cluster, and you can remove j from the j\u003Csup\u003Eth\u003C/sup\u003E cluster.\n\n    - Remove any entry from priority queue that contains j as target or source index.\n\n    - Update all entries in priority queue that contain i as source or targe index:\n\n      - j has to be added to every cluster that contains i\n\n      - Replace the distances with new distances of the merged mergedCluster to all other clusters.\n\n    - repeat cycle with next merge\n\n### 5\u003Csup\u003Eth\u003C/sup\u003E step:\n\n  - Clustering list now contains all possible cluster configurations. Convert the clustering list into\n  a binary tree structure such as \u0060\u0060\u0060ML.Unsupervised.HierarchicalClustering.Cluster\u003C\u0027a\u003E\u0060\u0060\u0060\n\n#### 6\u003Csup\u003Eth\u003C/sup\u003E step:\n\n  - Removing elements from the priority queue is slow. Is there a better way to avoid the deletion? \n  \n    - maybe a Map(int[],bool) would be beneficial\n\n    - or another implementation of heap/priority queues like C5.IntervalHeap could be faster\n\n## Goal/Additional information\n\n1. Introduction into hierarchical clustering:\n\n  - What is solved by the usage of hClust?\n  \n  - Classical application examples.\n  \n  - Limitations/Drawbacks of hClust.\n\n2. Implement hClust functions with parameters:\n\n    |Parameter name|data type|description|\n    |--------------|---------|-----------|\n    |data|\u0060\u0060\u0060seq\u003C\u0027a\u003E\u0060\u0060\u0060|data|\n    |distFu|\u0060\u0060\u0060\u0027a-\u003E\u0027a-\u003Efloat\u0060\u0060\u0060|distance Function from \u0060\u0060\u0060FSharp.Stats.ML.DistanceMetrics\u0060\u0060\u0060|\n    |linkageType|\u0060\u0060\u0060Linker.LancWilliamsLinker\u0060\u0060\u0060 or self defined|linkage type that is used during clustering|\n    ||||\n    |output|\u0060\u0060\u0060ML.Unsupervised.HierarchicalClustering.Cluster\u003C\u0027a\u003E\u0060\u0060\u0060 or cluster configuration list||\n\n3. Short description of the algorithm: \n\n  - maybe with flowchart visualization\n\n4. Test the algorithm with a data set of your choice.\n\n5. If you have any questions mail to venn@bio.uni-kl.de.\n\n*)"},{"uri":"/BIO-BTE-10-L-4/projects/tSNE.html","title":"t-Distributed Stochastic Neighbour Embedding","content":"(**\n---\ntitle: t-Distributed Stochastic Neighbour Embedding\ncategory: projects\ncategoryindex: 1\nindex: 1\n---\n*)\n\n\n(**\n# t-Distributed Stochastic Neighbour Embedding (tSNE)\n\n## Content\n\n1.\tIntroduction\n\n2.\tCoding clues\n\n3.\tReferences\n\n4.\tGoal/Additional information\n\n## Introduction\n\n  - tSNE is a dimensionality reduction method. It allows you to visualise a multi-dimensional dataset in 2 or 3 dimensional scatter plot. \nBut what does that mean in practice? Imagine you measured height, weight, width, density, brightness, as well as magnetic, chemical, \nand physical properties of a bunch of objects. The simplest technique to summarize your measurements is a spreadsheet table in which each \nrow represents an element, and each column represents a measured feature:\n\n\n  |Object ID|height|weight|width|density|brightness|magnetic field|...|\n  |---------|------|------|-----|-------|----------|--------------|---|\n  |objectA|2|30|3|2|200|100000|...|\n  |objectB|4|50|2|3|255|130000|...|\n  |objectC|15|20|1|2|11|10000000|...|\n  |...|...|...|...|...|...|...|...|\n  \n  \n\n  - Note that the measured features span multiple orders of magnitude. A change of 1 in height for example has much more value than a change \nof 1 regarding the magnetic field. If now clusters of similar behaving objects should be identified, you are limited to inspect the data set \ncolumn-wise by repetitive sorting. Just from the table you cannot create a meaningful graph, that allows you to perform a visual inspection of all features at once. \nLike principal component analysis (PCA), tSNE is a method for dimensionality reduction. It aggregates all features to a feature subset that \nallows a visual inspection of the complex data. It often is applied in image processing, NLP, genomic data, and speech processing. \n  \n  \n  ![](../img/tSNE.png)\n  Fig. 1: Idea of tSNE. Visualisation of a high dimensional data on a 2-dimensional scatter plot. \n  \n  \n## Coding Clues\n\n### Notes:\n\n  - All functions below are taken from the original publication (van der Maaten and Hinton 2008).\n\n  - Be aware, that the original work first describes SNE and later (section 3) describes the differences made to result in t-SNE!\n\n  - Although variance is continually referred to as \u03C3\u003Csub\u003Ei\u003C/sub\u003E in the paper, that is a repeated typo and should be \u03C3\u003Csub\u003Ei\u003C/sub\u003E\u003Csup\u003E2\u003C/sup\u003E.\n\n  - The data matrix has n rows (without header row). The first index defines the row, the second the column!\n\n  - x\u003Csub\u003Ei\u003C/sub\u003E defines the i\u003Csup\u003Eth\u003C/sup\u003E row in the data matrix (a vector of measured features).\n\n  - ||x|| indicates the vector norm, in this case it is the Euclidean distance between vector x\u003Csub\u003Ei\u003C/sub\u003E and y\u003Csub\u003Ei\u003C/sub\u003E. You can find distance metrics at \u0060\u0060\u0060FSharp.Stats.ML.DistanceMetrics\u0060\u0060\u0060.\n\n  - exp(t) indicates e\u003Csup\u003Et\u003C/sup\u003E\n\n  - A t distribution with degree of freedom = 1 is equal to 0.3183*(1\u002Bt\u00B2)-1 where the first constant part can be neglected if the constant term exists in all calculations.\n\n#### 0\u003Csup\u003Eth\u003C/sup\u003E step: \n\n  - Read the publication and visit further introduction material you can find below (References)\n  \n#### 1\u003Csup\u003Est\u003C/sup\u003E step: \n\n  - create a F# script (.fsx), load and open \u0060\u0060\u0060FSharp.Stats\u0060\u0060\u0060, \u0060\u0060\u0060FSharpAux\u0060\u0060\u0060, and \u0060\u0060\u0060Plotly.NET\u0060\u0060\u0060\n\n  - import test data\n\n    - You can find the classic clustering dataset \u0022iris\u0022 [here](https://github.com/fslaborg/FSharp.Stats/tree/developer/docs/data).\n\n*)\n\n(******)\n\n#r \u0022nuget: FSharp.Stats, 0.4.1\u0022\n#r \u0022nuget: Plotly.NET, 2.0.0-beta9\u0022\n            \nopen FSharp.Stats\nopen Plotly.NET\n\n\nlet fromFileWithSep (separator:char) (filePath) =     \n    // The function is implemented using a sequence expression\n    seq {   let sr = System.IO.File.OpenText(filePath)\n            while not sr.EndOfStream do \n                let line = sr.ReadLine() \n                let words = line.Split separator//[|\u0027,\u0027;\u0027 \u0027;\u0027\\t\u0027|] \n                yield words }\n\n                \nlet lables,data =\n    fromFileWithSep \u0027,\u0027 (__SOURCE_DIRECTORY__ \u002B \u0022../content/irisData.csv\u0022)\n    |\u003E Seq.skip 1\n    |\u003E Seq.map (fun arr -\u003E arr.[4], [| float arr.[0]; float arr.[1]; float arr.[2]; float arr.[3]; |])\n    |\u003E Seq.toArray\n    |\u003E Array.shuffleFisherYates\n    |\u003E Array.mapi (fun i (lable,data) -\u003E sprintf \u0022%s_%i\u0022 lable i, data)\n    |\u003E Array.unzip\n\n\n(**\n\n#### 1\u003Csup\u003Est\u003C/sup\u003E step:\n\n  - Calculate a Euclidean distance matrix using \u0060\u0060\u0060ML.DistanceMetrics.euclidean\u0060\u0060\u0060. The matrix\u2019 dimensions are n x n.\n  \n  - Define functions that calculate similarity measures using the prior defined distance matrix:\n    \n    - (1) high dimensional affinity p (p\u003Csub\u003Ei|j\u003C/sub\u003E)(Equation 1)\n\n      - Inform yourself how the variance is determined. If required define a Perplexity beforehand.\n\n    - (2) low dimensional affinity q (q\u003Csub\u003Eij\u003C/sub\u003E) (Equation 4)\n\n\n#### 2\u003Csup\u003End\u003C/sup\u003E step: \n\n  - Calculate the high dimensional affinity matrix between every data pair.\n\n    - Note: p\u003Csub\u003Eij\u003C/sub\u003E \u2260 p\u003Csub\u003Ei|j\u003C/sub\u003E\n\n    - p\u003Csub\u003Eij\u003C/sub\u003E = (p\u003Csub\u003Ej|i\u003C/sub\u003E \u002B p\u003Csub\u003Ei|j\u003C/sub\u003E) / 2n\n\n  - The matrix has the dimensions n x n . The similarity of a point to itself is 0.\n\n\n#### 3\u003Csup\u003Erd\u003C/sup\u003E step: \n\n  - Create an initial solution y(0) so that:\n\n    - y(0) is a matrix (n x d)\n\n    - y(0) contains as many rows as the original data matrix has rows (n)\n\n    - The number of values in each row is the number of dimensions you want to obtain in the end (d; in most cases 1-3, but should be defined by user).\n\n    - Each value is a randomly sampled from a normal distribution with mean = 0 and var = 0.0001.\n\n*)\n\n(******)\n// defines a normal distribuiton with mean = 3 and stDev = 2\nlet normalDist = Distributions.Continuous.normal 3. 2.\n\nlet createInitialGuess n = Array.init n (fun x -\u003E normalDist.Sample())\n\n// see FSharp.Stats documentation for probability distributions in the first code block for details\n// https://fslab.org/FSharp.Stats/Distributions.html#Normal-distribution)\n\n\n(**\n\n\n#### 4\u003Csup\u003Eth\u003C/sup\u003E step:\n\n  - Recursively loop from t=1 to T (number of iterations)\n\n\n  - calculate low dimensional affinities (q\u003Csub\u003Eij\u003C/sub\u003E (Equation 4)) for all low dimensional result vectors from 3\u003Csup\u003Erd\u003C/sup\u003E step. Collect results in a matrix (n x n).\n\n  - compute gradient (Equation 5)\n\n  - calculate the updated result y(t) and repeat.\n\n\n#### 5\u003Csup\u003Eth\u003C/sup\u003E step:\n\n  - report y(T) as final result\n\n\n#### 6\u003Csup\u003Eth\u003C/sup\u003E step:\n\n  - Use a 2D and 3D scatter plot from Plotly.NET to visualize your result.\n\n\n### Pseudocode:\n\n![](../img/tSNE_pc.png)\n\n\n## References\n\n  - original work: https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf \n\n  - https://cran.r-project.org/web/packages/Rtsne/Rtsne.pdf page 5\n\n  - https://www.analyticsvidhya.com/blog/2017/01/t-sne-implementation-r-python/\n\n    - Note: Inform yourself if the variance in Step 4 is in fact based on t distribution or if at this point of the algorithm a standard gaussian normal distribution is used!\n\n  - https://www.datacamp.com/community/tutorials/introduction-t-sne\n\n  - https://www.youtube.com/watch?v=NEaUSP4YerM \n\n\n## Additional information\n\nAim for this project: \n\n1. Blog post introducing the method, its applications, and limitations.\n\n  - Don\u2019t forget to describe the limits/weaknesses of the approach in your blog post.\n\n  - How to handle/preprocess ties?\n\n2. Function with parameters (suggestion):\n    \n  |Parameter name|data type|description|\n  |--------------|---------|-----------|\n  |data|\u0060\u0060\u0060matrix\u0060\u0060\u0060|datamatrix (cols=features, rows=elements)|\n  |dimensions|\u0060\u0060\u0060int\u0060\u0060\u0060|number of dimensions the final output data points have|\n  |maxIter|\u0060\u0060\u0060int\u0060\u0060\u0060|maximal number of iterations|\n  |perplexity|\u0060\u0060\u0060float\u0060\u0060\u0060|inform yourself if the perplexity should be defined by the user, or is calculated within the algorithm|\n  |learnRate|\u0060\u0060\u0060float\u0060\u0060\u0060|inform yourself|\n  |momentum|\u0060\u0060\u0060float\u0060\u0060\u0060|inform yourself|\n\n  - Define default parameters.\n\n3. Apply tSNE to a dataset of your choice.\n\n4. Test your results against implementations in R/Python or in the best case against the datasets proposed in the original publication.\n\n5. Optional: Compare the method to PCA.\n\n6. If you have any questions mail to venn@bio.uni-kl.de.\n\n*)\n\n"}]